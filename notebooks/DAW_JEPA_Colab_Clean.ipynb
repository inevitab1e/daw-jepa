{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdb740b",
   "metadata": {},
   "source": [
    "# DAW-JEPA – Colab Repro Notebook\n",
    "\n",
    "This notebook shows **step-by-step how to run my DAW-JEPA project code on Google Colab**:\n",
    "\n",
    "1. Set up the runtime and clone the GitHub repo.\n",
    "2. Prepare the ImageNet-100 data used for pretraining.\n",
    "3. Run I-JEPA / DAW-JEPA pretraining on ImageNet-100.\n",
    "4. Run linear probing on STL10 / CIFAR-10.\n",
    "5. Run k-NN evaluation on STL10 / CIFAR-10.\n",
    "\n",
    "> Note: This notebook is designed to be self-contained and readable for grading. > It is not meant to be perfectly resource-efficient – full pretraining on ImageNet-100 > is still compute-heavy, even on Colab GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01172b24",
   "metadata": {},
   "source": [
    "## 0. Runtime & environment\n",
    "\n",
    "Please make sure the Colab runtime has a **GPU** attached:\n",
    "\n",
    "- Runtime → Change runtime type → Hardware accelerator → GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb62e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi || echo \"No GPU found. Please enable GPU in Colab runtime settings.\"\n",
    "\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50c8af",
   "metadata": {},
   "source": [
    "## 1. Clone the GitHub repo\n",
    "\n",
    "Replace `<YOUR_GITHUB_USERNAME>` with my GitHub username if needed.\n",
    "The repo corresponds exactly to the code submitted for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86283aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the DAW-JEPA repo (GitHub URL to be updated by me before submission)\n",
    "%cd /content\n",
    "!git clone https://github.com/<YOUR_GITHUB_USERNAME>/daw-jepa.git\n",
    "%cd daw-jepa\n",
    "\n",
    "# List top-level files for sanity check\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8dae29",
   "metadata": {},
   "source": [
    "## 2. Install additional Python packages (if needed)\n",
    "\n",
    "Colab already comes with `torch` and `torchvision`.  \n",
    "Here we only install **lightweight utilities** that may not be pre-installed.\n",
    "\n",
    "If the following cell fails because a package is already installed, it is safe to ignore the warning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716efde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install utilities used in this project\n",
    "!pip install -q pyyaml pandas matplotlib kagglehub wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd059ba9",
   "metadata": {},
   "source": [
    "## 3. Prepare the ImageNet-100 dataset\n",
    "\n",
    "The pretraining in this project uses **ImageNet-100**.\n",
    "\n",
    "Because of licensing, the dataset itself is **not included** in the repo.  \n",
    "This notebook assumes **one** of the following:\n",
    "\n",
    "1. You already have `imagenet100.zip` stored in your Google Drive at:\n",
    "   - `/content/drive/MyDrive/imagenet100.zip`, or\n",
    "2. You adapt the paths below to your own ImageNet-100 location.\n",
    "\n",
    "In my experiments, I used Option 1. The following cells reproduce that setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d070427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Mount Google Drive (required if dataset is stored there)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Unzip ImageNet-100 from Google Drive into the Colab runtime\n",
    "# If your zip file is in a different location, please update the path below.\n",
    "!unzip -q /content/drive/MyDrive/imagenet100.zip -d /content/imagenet100_local\n",
    "\n",
    "# After this, the expected directory structure is:\n",
    "# /content/imagenet100_local/train/<class_name>/*.JPEG\n",
    "# /content/imagenet100_local/val/<class_name>/*.JPEG\n",
    "\n",
    "!ls /content/imagenet100_local || echo \"Please check that imagenet100.zip exists in your Drive.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a6790",
   "metadata": {},
   "source": [
    "### 3.3 Verify config data paths\n",
    "\n",
    "The training configs used in this project expect:\n",
    "\n",
    "```yaml\n",
    "data:\n",
    "  root_path: /content\n",
    "  image_folder: imagenet100_local\n",
    "```\n",
    "\n",
    "This matches the directory layout created above (`/content/imagenet100_local`).  \n",
    "If you change the data path, please update the YAML configs under `configs/` accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52a02a",
   "metadata": {},
   "source": [
    "## 4. Run pretraining (I-JEPA and DAW-JEPA)\n",
    "\n",
    "This section shows how to:\n",
    "\n",
    "- Train a **baseline I-JEPA** model on ImageNet-100.\n",
    "- Train **DAW-JEPA** variants (EMA and Instant modes).\n",
    "\n",
    "All commands run from the repo root (`/content/daw-jepa`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05abd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the repo root\n",
    "%cd /content/daw-jepa\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9745012",
   "metadata": {},
   "source": [
    "### 4.1 Baseline I-JEPA on ImageNet-100 (optional but recommended)\n",
    "\n",
    "This uses the original I-JEPA objective without difficulty-aware weighting.\n",
    "\n",
    "Config file: `configs/in100_vits16_ep100.yaml`\n",
    "\n",
    "> Note: Full training for 100 epochs is compute-heavy. For a quick smoke test, > you can reduce `num_epochs` in the config or stop the run early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline I-JEPA pretraining on ImageNet-100\n",
    "!python main.py --fname configs/in100_vits16_ep100.yaml --devices cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8930e",
   "metadata": {},
   "source": [
    "### 4.2 DAW-JEPA (EMA mode)\n",
    "\n",
    "This enables **difficulty-aware weighting** with an EMA difficulty buffer.\n",
    "\n",
    "Config file: `configs/in100_vits16_ep100_daw_ema.yaml`\n",
    "\n",
    "Key DAW settings (inside the YAML):\n",
    "\n",
    "```yaml\n",
    "daw:\n",
    "  enabled: true\n",
    "  mode: ema\n",
    "  ema_alpha: 0.7\n",
    "  gamma: 0.3\n",
    "  w_min: 0.8\n",
    "  w_max: 1.2\n",
    "  warmup_epochs: 20\n",
    "```\n",
    "\n",
    "The command below starts pretraining with these settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAW-JEPA pretraining (EMA difficulty)\n",
    "!python main.py --fname configs/in100_vits16_ep100_daw_ema.yaml --devices cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73195a",
   "metadata": {},
   "source": [
    "### 4.3 DAW-JEPA (Instant mode)\n",
    "\n",
    "This variant updates the difficulty buffer **instantly** with the current loss instead of using EMA.\n",
    "\n",
    "Config file: `configs/in100_vits16_ep100_daw_instant.yaml`\n",
    "\n",
    "Example DAW settings:\n",
    "\n",
    "```yaml\n",
    "daw:\n",
    "  enabled: true\n",
    "  mode: instant\n",
    "  gamma: 0.3\n",
    "  w_min: 0.8\n",
    "  w_max: 1.2\n",
    "  warmup_epochs: 10   # used in my best CIFAR-10 run\n",
    "```\n",
    "\n",
    "Run the command below to launch Instant-mode DAW-JEPA pretraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAW-JEPA pretraining (Instant difficulty)\n",
    "!python main.py --fname configs/in100_vits16_ep100_daw_instant.yaml --devices cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce8ac3",
   "metadata": {},
   "source": [
    "### 4.4 Locate the latest checkpoints\n",
    "\n",
    "Each run writes checkpoints and logs under the `logs/` directory.\n",
    "\n",
    "The following helper cell prints all `jepa-latest.pth.tar` checkpoints and lets you pick one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "ckpts = sorted(glob.glob(\"logs/**/jepa-latest.pth.tar\", recursive=True))\n",
    "print(\"Found checkpoints:\")\n",
    "for i, p in enumerate(ckpts):\n",
    "    print(f\"[{i}] {p}\")\n",
    "\n",
    "# For convenience, you can select one index here:\n",
    "SELECTED = -1  # -1 means \"last one\"; or set to 0, 1, 2, ...\n",
    "\n",
    "if ckpts:\n",
    "    CKPT_PATH = ckpts[SELECTED]\n",
    "    print(\"\\nUsing CKPT_PATH =\", CKPT_PATH)\n",
    "else:\n",
    "    CKPT_PATH = None\n",
    "    print(\"No checkpoints found. Please run a pretraining cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbfad8",
   "metadata": {},
   "source": [
    "## 5. Linear probing on STL10 / CIFAR-10\n",
    "\n",
    "This section evaluates the **frozen encoder** using a linear classifier.\n",
    "\n",
    "The main script is `src/linprobe.py`.  \n",
    "Datasets (STL10 / CIFAR-10) will be downloaded automatically under `./data` by torchvision.\n",
    "\n",
    "Before running, please make sure:\n",
    "\n",
    "- `CKPT_PATH` above points to a valid `jepa-latest.pth.tar` checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity-check CKPT_PATH\n",
    "print(\"Current CKPT_PATH:\", CKPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b53547",
   "metadata": {},
   "source": [
    "### 5.1 STL10 linear probe\n",
    "\n",
    "This trains a linear classifier on top of the frozen JEPA encoder using STL10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear probe on STL10\n",
    "!python -m src.linprobe \\\n",
    "  --ckpt_path \"$CKPT_PATH\" \\\n",
    "  --dataset stl10 \\\n",
    "  --data_root ./data \\\n",
    "  --crop_size 224 \\\n",
    "  --batch_size 256 \\\n",
    "  --epochs 100 \\\n",
    "  --lr 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa4d59",
   "metadata": {},
   "source": [
    "### 5.2 CIFAR-10 linear probe\n",
    "\n",
    "This trains a linear classifier on CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear probe on CIFAR-10\n",
    "!python -m src.linprobe \\\n",
    "  --ckpt_path \"$CKPT_PATH\" \\\n",
    "  --dataset cifar10 \\\n",
    "  --data_root ./data \\\n",
    "  --crop_size 224 \\\n",
    "  --batch_size 256 \\\n",
    "  --epochs 100 \\\n",
    "  --lr 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ac725",
   "metadata": {},
   "source": [
    "## 6. k-NN evaluation on STL10 / CIFAR-10\n",
    "\n",
    "This section evaluates the frozen encoder using a **non-parametric k-NN classifier**.\n",
    "\n",
    "Script: `src.knn_eval`\n",
    "\n",
    "The following commands reproduce the evaluations I reported in the project write-up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d6512",
   "metadata": {},
   "source": [
    "### 6.1 k-NN on STL10\n",
    "\n",
    "Uses k = 20 by default (can be changed via `--k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e798313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN evaluation on STL10\n",
    "!python -m src.knn_eval \\\n",
    "  --ckpt_path \"$CKPT_PATH\" \\\n",
    "  --data_root ./data \\\n",
    "  --dataset stl10 \\\n",
    "  --k 20 \\\n",
    "  --batch_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbb20b",
   "metadata": {},
   "source": [
    "### 6.2 k-NN on CIFAR-10\n",
    "\n",
    "Uses k = 50 by default (can be changed via `--k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a876006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN evaluation on CIFAR-10\n",
    "!python -m src.knn_eval \\\n",
    "  --ckpt_path \"$CKPT_PATH\" \\\n",
    "  --data_root ./data \\\n",
    "  --dataset cifar10 \\\n",
    "  --k 50 \\\n",
    "  --batch_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269fb56",
   "metadata": {},
   "source": [
    "## 7.  DAW hyper-parameter sweep\n",
    "\n",
    "The script `sweep.py` generates DAW configs from a base YAML and runs multiple pretraining jobs\n",
    "with different `daw.mode` / `daw.gamma` settings.\n",
    "\n",
    "Because this can be computationally expensive, this section is **optional** for reproducing the main results,\n",
    "but is included here for completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a DAW sweep (optional; may take a long time)\n",
    "# This will use the base config inside sweep.py and generate configs under configs/sweep/\n",
    "!python sweep.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
